{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\"\"\"\n",
    "循环神经网络 RNN\n",
    "在这里的实现是  一个时间步一个时间步  来训练的\n",
    "\"\"\"\n",
    "\n",
    "# 迭代器每次给出 batch_size 组数据\n",
    "# 每一组中单个序列的长度为 num_steps \n",
    "batch_size, num_steps = 32, 35\n",
    "\n",
    "# 每次从 train_iter 中取出一个元素\n",
    "# 元组包含两个元素一个是 X 一个是 Y\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[ 1,  3,  5,  ...,  5, 13,  2],\n",
      "        [18,  9,  3,  ..., 20,  4,  8],\n",
      "        [ 9,  1,  4,  ..., 11,  1, 12],\n",
      "        ...,\n",
      "        [ 1, 15,  7,  ...,  1, 21, 14],\n",
      "        [10, 19,  8,  ..., 14,  8,  3],\n",
      "        [13,  2, 15,  ...,  1,  4,  6]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[ 3,  5, 13,  ..., 13,  2,  1],\n",
      "        [ 9,  3,  1,  ...,  4,  8,  8],\n",
      "        [ 1,  4,  1,  ...,  1, 12,  4],\n",
      "        ...,\n",
      "        [15,  7,  6,  ..., 21, 14,  3],\n",
      "        [19,  8,  3,  ...,  8,  3,  1],\n",
      "        [ 2, 15,  9,  ...,  4,  6, 11]])\n",
      "tensor([ 3,  9,  1,  ...,  3,  1, 11])  and y.long() is  tensor([ 3,  9,  1,  ...,  3,  1, 11])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[ 1,  3, 10,  ..., 22,  2,  6],\n",
      "        [ 8,  2, 11,  ...,  5,  6, 18],\n",
      "        [ 4, 25,  5,  ..., 10,  1,  3],\n",
      "        ...,\n",
      "        [ 3, 21,  2,  ...,  4, 15,  2],\n",
      "        [ 1, 21,  2,  ...,  8,  1, 14],\n",
      "        [11,  1,  8,  ...,  1,  7,  6]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[ 3, 10,  4,  ...,  2,  6,  5],\n",
      "        [ 2, 11,  1,  ...,  6, 18,  1],\n",
      "        [25,  5, 12,  ...,  1,  3,  9],\n",
      "        ...,\n",
      "        [21,  2, 16,  ..., 15,  2,  1],\n",
      "        [21,  2,  1,  ...,  1, 14,  6],\n",
      "        [ 1,  8,  4,  ...,  7,  6,  1]])\n",
      "tensor([ 3,  2, 25,  ...,  1,  6,  1])  and y.long() is  tensor([ 3,  2, 25,  ...,  1,  6,  1])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[ 5,  2,  6,  ...,  6, 18,  1],\n",
      "        [ 1,  9,  5,  ..., 11,  1, 14],\n",
      "        [ 9,  5,  8,  ...,  4,  6, 11],\n",
      "        ...,\n",
      "        [ 1,  3,  9,  ..., 11, 16,  5],\n",
      "        [ 6, 12,  2,  ..., 15, 15,  2],\n",
      "        [ 1,  3,  9,  ..., 20,  1,  3]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[ 2,  6,  3,  ..., 18,  1,  4],\n",
      "        [ 9,  5,  8,  ...,  1, 14,  8],\n",
      "        [ 5,  8,  1,  ...,  6, 11,  1],\n",
      "        ...,\n",
      "        [ 3,  9,  2,  ..., 16,  5, 12],\n",
      "        [12,  2,  8,  ..., 15,  2, 20],\n",
      "        [ 3,  9,  2,  ...,  1,  3,  9]])\n",
      "tensor([ 2,  9,  5,  ..., 12, 20,  9])  and y.long() is  tensor([ 2,  9,  5,  ..., 12, 20,  9])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[ 4,  1, 10,  ..., 19,  1,  2],\n",
      "        [ 8,  1, 10,  ...,  3,  1, 14],\n",
      "        [ 1,  9,  5,  ...,  1, 15,  4],\n",
      "        ...,\n",
      "        [12, 21, 19,  ..., 18,  1,  3],\n",
      "        [20,  3,  2,  ..., 12,  2,  1],\n",
      "        [ 9,  2,  1,  ...,  7,  6,  3]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[ 1, 10,  2,  ...,  1,  2, 19],\n",
      "        [ 1, 10,  4,  ...,  1, 14, 20],\n",
      "        [ 9,  5,  8,  ..., 15,  4, 10],\n",
      "        ...,\n",
      "        [21, 19,  1,  ...,  1,  3,  9],\n",
      "        [ 3,  2, 11,  ...,  2,  1,  3],\n",
      "        [ 2,  1, 21,  ...,  6,  3,  9]])\n",
      "tensor([1, 1, 9,  ..., 9, 3, 9])  and y.long() is  tensor([1, 1, 9,  ..., 9, 3, 9])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[19,  2,  8,  ..., 14,  4, 12],\n",
      "        [20,  7,  6,  ...,  4, 16,  3],\n",
      "        [10,  2, 16,  ..., 10,  3,  1],\n",
      "        ...,\n",
      "        [ 9,  2,  1,  ...,  6, 11,  1],\n",
      "        [ 3,  9,  5,  ..., 15,  3,  4],\n",
      "        [ 9,  2,  1,  ...,  1,  4,  1]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[ 2,  8,  1,  ...,  4, 12, 12],\n",
      "        [ 7,  6,  1,  ..., 16,  3,  2],\n",
      "        [ 2, 16, 14,  ...,  3,  1,  7],\n",
      "        ...,\n",
      "        [ 2,  1,  3,  ..., 11,  1, 17],\n",
      "        [ 9,  5,  6,  ...,  3,  4, 18],\n",
      "        [ 2,  1, 13,  ...,  4,  1, 11]])\n",
      "tensor([ 2,  7,  2,  ..., 17, 18, 11])  and y.long() is  tensor([ 2,  7,  2,  ..., 17, 18, 11])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[12, 19,  1,  ..., 13,  4,  3],\n",
      "        [ 2, 10,  1,  ...,  3,  1, 10],\n",
      "        [ 7,  6,  2,  ..., 14,  6,  5],\n",
      "        ...,\n",
      "        [17,  4,  8,  ..., 10, 23,  1],\n",
      "        [18,  7,  6,  ...,  4, 21,  7],\n",
      "        [11,  7, 25,  ..., 15,  4,  6]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[19,  1, 20,  ...,  4,  3,  2],\n",
      "        [10,  1, 11,  ...,  1, 10,  7],\n",
      "        [ 6,  2,  1,  ...,  6,  5, 22],\n",
      "        ...,\n",
      "        [ 4,  8,  1,  ..., 23,  1,  8],\n",
      "        [ 7,  6,  4,  ..., 21,  7, 14],\n",
      "        [ 7, 25,  2,  ...,  4,  6, 11]])\n",
      "tensor([19, 10,  6,  ...,  8, 14, 11])  and y.long() is  tensor([19, 10,  6,  ...,  8, 14, 11])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[ 2, 11,  1,  ...,  9,  2,  1],\n",
      "        [ 7,  4, 13,  ...,  2, 12,  8],\n",
      "        [22,  2, 10,  ...,  7, 10,  5],\n",
      "        ...,\n",
      "        [ 8, 15,  4,  ..., 15, 23,  1],\n",
      "        [14,  3,  1,  ...,  7, 16,  1],\n",
      "        [11, 12,  2,  ..., 22,  2, 10]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[11,  1,  3,  ...,  2,  1,  8],\n",
      "        [ 4, 13,  8,  ..., 12,  8,  1],\n",
      "        [ 2, 10,  8,  ..., 10,  5,  6],\n",
      "        ...,\n",
      "        [15,  4, 10,  ..., 23,  1,  4],\n",
      "        [ 3,  1,  3,  ..., 16,  1,  3],\n",
      "        [12,  2,  8,  ...,  2, 10,  4]])\n",
      "tensor([11,  4,  2,  ...,  4,  3,  4])  and y.long() is  tensor([11,  4,  2,  ...,  4,  3,  4])\n",
      "------------------\n",
      "Matrix X is  torch.Size([32, 35])\n",
      "tensor([[ 8,  7, 16,  ...,  3, 12,  5],\n",
      "        [ 1,  7, 16,  ...,  8,  1,  5],\n",
      "        [ 6,  8,  3,  ..., 12,  1,  5],\n",
      "        ...,\n",
      "        [ 4,  6, 11,  ...,  8,  1,  5],\n",
      "        [ 3,  9,  2,  ..., 10,  3,  9],\n",
      "        [ 4, 12,  1,  ...,  8,  1, 21]])\n",
      "Matrix Y is  torch.Size([32, 35])\n",
      "tensor([[ 7, 16,  3,  ..., 12,  5, 18],\n",
      "        [ 7, 16,  1,  ...,  1,  5,  6],\n",
      "        [ 8,  3,  4,  ...,  1,  5,  8],\n",
      "        ...,\n",
      "        [ 6, 11,  1,  ...,  1,  5, 22],\n",
      "        [ 9,  2,  1,  ...,  3,  9, 10],\n",
      "        [12,  1,  5,  ...,  1, 21, 10]])\n",
      "tensor([ 7,  7,  8,  ..., 22, 10, 10])  and y.long() is  tensor([ 7,  7,  8,  ..., 22, 10, 10])\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_iter:\n",
    "    print('Matrix X is ', X.shape)\n",
    "    print(X)\n",
    "    print('Matrix Y is ', Y.shape)\n",
    "    print(Y)\n",
    "    # y 是 Y.T 拉直后的样子，顺序拼接\n",
    "    # 相对与 Y 来说就是把 Y 的每一列依次拼接成 1 维的向量\n",
    "    y = Y.T.reshape(-1)\n",
    "    print(y, ' and y.long() is ', y.long())\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot 编码就是 概率分布\n",
    "# 0 号元素是正确的，所以第 0 号元素的概率为 1 \n",
    "# 2 号元素是正确的，所以第 2 号元素的概率为 1\n",
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 小批量数据的形状是 （批量大小，时间步数）\n",
    "# 28 代表种类有 28 种,在对应位置上才为 1\n",
    "# 共有 2 个序列，每个序列的长度为 5\n",
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置后形状变成了 （时间步数，批量大小）\n",
    "# 5 行 ==> 5 个时间点\n",
    "# 2 列 ==> 2 个序列\n",
    "# 从 ont-hot 角度看\n",
    "# 它把每行的 2 个元素都展开成了 28 位长的 ont-hot 数组\n",
    "\n",
    "# (时间步数，批量大小，ont-hot 数组长度)\n",
    "F.one_hot(X.T, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 小批量数据的形状是 （批量大小，时间步数）\n",
    "# 28 代表种类有 28 种,在对应位置上才为 1\n",
    "X = torch.arange(4).reshape((2, 2))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(10).reshape((2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        # 初始化学习参数\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    # print('W_xh.shape is ', W_xh.shape)\n",
    "\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    # print('W_hh.shape is ', W_hh.shape)\n",
    "\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # print('b_h.shape is ', b_h.shape)\n",
    "\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    # print('W_hq.shape is ', W_hq.shape)\n",
    "\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # print('b_q.shape is ', b_q.shape)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化隐藏状态\n",
    "# 虽然第一次的 state 被初始化为了 0 矩阵\n",
    "# 但是下一次的 state 的计算就不是 0 矩阵了\n",
    "# 会经过加和的过程计算出来下一次的 state\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_xh.shape is  torch.Size([28, 512])\n",
    "# W_hh.shape is  torch.Size([512, 512])\n",
    "# b_h.shape is  torch.Size([512])\n",
    "# W_hq.shape is  torch.Size([512, 28])\n",
    "# b_q.shape is  torch.Size([28])\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs 是从数据迭代器中取到的数据 X.T 的 ont-hot 编码后的样子\n",
    "    # print('inputs.shape is ', inputs.shape)\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    # 时间步数是 一个序列的长度\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    # state 是一个长度为 1 的 tuple\n",
    "    H, = state\n",
    "    # print('H.shape is ', H.shape)\n",
    "    outputs = []\n",
    "    # X 的形状：(批量大小，词表大小)  词表大小就是共有多少个 词元，不重复的分词后的数量\n",
    "    # X 是整个批量中所有序列的在某个时间步上的 ont-hot 编码，也即特定于某一个输出的概率分布\n",
    "    # 这样就特定训练了某一个输出，而且给定了其他的输出应有的值\n",
    "    # 每一轮循环中计算的是为了计算某一个特定时间步上的预测值\n",
    "    for X in inputs:\n",
    "        # 每次拿到的 X 是一个时刻上的所有样本的训练集(批量大小，词表大小) \n",
    "\n",
    "        # print('X.shape is ', X.shape)\n",
    "        # W_xh 的每一列是一个 神经元 ==> 共有 h 个神经元\n",
    "        # H 的形状是 （批量大小，隐藏层个数）\n",
    "        # X 的形状是 （批量大小，时间步数）\n",
    "        # (X, W_xh) （X 的行， W_xh 的列 隐藏层大小）\n",
    "        # (H, W_hh) （H 的行， W_hh 的列 隐藏层大小）\n",
    "        # 这里的 X 只是一个时间节点上的取值，计算出来下一次的 H\n",
    "        # W_hh 存储信息，而 H 用来表示上一次的状态\n",
    "        # 所以 H 是动态得出的，不需要对 H 进行存储\n",
    "        # H 是多个 batch_size 个样本的在 X 所表示的 batch_size 个样本的当前上一次的状态取值\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        # 如果 X 中只含有一个样本\n",
    "        # 那么 Y 就是一个行向量\n",
    "        # 如果 X 中有多个样本\n",
    "        # 那么 Y 是一个矩阵，并且 Y 的每一行是一个样本在某个时间步的概率分布\n",
    "        outputs.append(Y)\n",
    "        # print('Matrix Y is ')\n",
    "        # print(Y)\n",
    "        # print('Y.shape is ', Y.shape)\n",
    "        # print('---------------------------------- relate inputs ----------------')\n",
    "    res = torch.cat(outputs, dim=0)\n",
    "    # print('res.shape is ', res)\n",
    "    return res, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "                 \n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        # 参数 X.shape is (batch_size, num_steps) (批量大小，序列长度:时间步数)\n",
    "        # vocab 存储了 数值 到 字符串 的对应关系，包含一些特殊字符串\n",
    "        # 按照英文字母预测则 词表大小 = 26 + 特殊字符\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "torch.Size([2, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查输出是否符合要求\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "print(X)\n",
    "print(state[0].shape)\n",
    "\n",
    "# 一个样本就需要一个 state 因此就需要 X.shape[0] 哥 state\n",
    "# 输入的数据 X 的形状 ==> 批量大小, 时间步数\n",
    "# print('输入的数据 X 的形状 X.shape is ', X.shape)\n",
    "\n",
    "# 对上一次的数据完成前向传播后产生一个新的 H\n",
    "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "\n",
    "# Y 中每 batch_size 行分别对应了不同的样本在某时间步上的 Y_hat ont-hot 编码\n",
    "# 一个 batch_size 这么多行是各个样本在同一个时间步上的 y_hat ont-hot \n",
    "# new_state 在这一小节是一个长度为 1 的 tuple\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"\n",
    "    在prefix后面生成新字符\n",
    "    num_preds: 后续要生成多少个 词（本节的次元是 字符）\n",
    "    \"\"\"\n",
    "    # batch_size = 1 ==> 对一个字符串做预测\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]  # 把 prefix[0] 对应的 数值映射 放进 outputs 中\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期 ==> 把 prefix 中的信息初始化到 state 中\n",
    "        # y 是从 prefix 的 第1号 元素开始取值的\n",
    "        # 当 y 取 prefix[1] 的时候，此时 get_input() 得到的输入是 prefix[0] \n",
    "        # 刚好是错开的，是没有问题的\n",
    "        # 之所以 state 也是网络的输入参数\n",
    "        # 就是使用 state 保存了 prefix 的信息，而 W_hh 是帮助生成 state 用的\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "\n",
    "    # 从这儿开始是预测 prefix 后边的字符\n",
    "    # 每次都用上一次输出的值作为下一次输入的值\n",
    "    # 上一次的输出值是一个时间步上的值，而不是一个序列\n",
    "    # 这刚好和在训练模型时的思路是一致的，使用了时间序列中的\n",
    "    # 某一个时间步上的值的 one-hot 编码作为输入\n",
    "    # 因此这里的 RNN 在真正预测的核心部分，它的输入依旧是时间序列中的\n",
    "    # 某个时间步的值，但用 ont-hot 值来表达\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "\n",
    "    # 把 index 的值 转化成其 对应的 词（这里是字符）\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller r r r r r '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save ch8 = chapter 8 第 8 章\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        # y_hat 是 y 的 one-hot 编码形式\n",
    "        # y 中的每一个元素展开成为 ont-hot 编码后得到 y_hat\n",
    "        # y 中的一个元素和 y_hat 的一个元素（ont-hot 向量） 做交叉熵损失\n",
    "        # pytorch 中的交叉熵损失函数会自动处理 ont-hot 编码计算损失\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "\n",
    "        # y 在这里是一个 1 维的向量\n",
    "        # y_hat 是一个 2 维的矩阵，并且是 y 的 ont-hot 编码形式\n",
    "\n",
    "        print('y_hat.shape is ', y_hat.shape, y_hat.dtype)\n",
    "        print('y.shape is ', y.shape, y.dtype)\n",
    "        print('y.long().shape is ', y.long().shape, y.long().dtype)\n",
    "        print('--------------------')\n",
    "\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 1, 1\n",
    "# train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 tensor 的 long 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1.1, 2.2, 3.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 2.2000, 3.3000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6],\n",
       "        [11, 22, 33, 44, 55, 66],\n",
       "        [ 6,  5,  4,  3,  2,  1],\n",
       "        [66, 55, 44, 33, 22, 11]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([\n",
    "    [1, 2, 3, 4, 5, 6],\n",
    "    [11, 22, 33, 44, 55, 66]\n",
    "])\n",
    "\n",
    "Y = torch.tensor([\n",
    "    [6, 5, 4, 3, 2, 1],\n",
    "    [66, 55, 44, 33, 22, 11]\n",
    "])\n",
    "\n",
    "torch.cat([X, Y], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 6]), torch.Size([3]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 这里可以为 6\n",
    "# 因为样本中不一定覆盖了所有的概率分布位置\n",
    "# 所以有些地方可以不被覆盖\n",
    "# 比如 [1, 3, 5]\n",
    "y = torch.tensor([1., 3., 5.])\n",
    "# y_hat = F.one_hot(y.long(), 6).float()\n",
    "\n",
    "y_hat = torch.tensor([[1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 1.],\n",
    "        [1., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "y_hat.shape, y.shape, y_hat.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9596)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_hat, y.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 lambda 表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "say_your_name = lambda name: print(f'I am {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 123\n"
     ]
    }
   ],
   "source": [
    "say_your_name('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:13:39) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86bb26bcf3701c754d5fd40db657c5b7e15a074c1099b32f8fc32052c18ad091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
