{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n",
    "                 **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0475,  0.3512,  0.0590,  0.1541,  0.6040, -0.2340, -0.1743,  0.4870],\n",
       "        [-1.0475,  0.3512,  0.0590,  0.1541,  0.6040, -0.2340, -0.1743,  0.4870],\n",
       "        [-1.0475,  0.3512,  0.0590,  0.1541,  0.6040, -0.2340, -0.1743,  0.4870]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn = PositionWiseFFN(4, 4, 8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2, 3, 4)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm: tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>) \n",
      "batch norm: tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)\n",
    "X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)\n",
    "# 在训练模式下计算X的均值和方差\n",
    "print('layer norm:', ln(X), '\\nbatch norm:', bn(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()\n",
    "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False, **kwargs):\n",
    "        \"\"\"\n",
    "        key_size: 指定输入到 Multi-Head-Attention 的 input 向量 经过一个 key_size 个数的线性层变为 key 向量\n",
    "        query_size: 同理\n",
    "        value_size: 同理\n",
    "        \"\"\"\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = d2l.MultiHeadAttention(\n",
    "            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n",
    "            use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(\n",
    "            ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 100, 24))\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(X, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class TransformerEncoder(d2l.Encoder):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        \"\"\"\n",
    "        key_size: 指定输入到 Multi-Head-Attention 的 input 向量 经过一个 key_size 个数的线性层变为 key 向量\n",
    "        query_size: 同理\n",
    "        value_size: 同理\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_hiddens = num_hiddens\n",
    "        # vocab_size 是在 embedding 层中进行 one-hot 编码步骤用到的\n",
    "        # num_hiddens 是把输入到 embedding 的最后一个维度的每个元素展开成 num_hiddens 长的向量\n",
    "        # num_hiddens 代表每个词元 使用 num_hiddens 长度的向量表示\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # 对输入的数据添加位置编码\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                EncoderBlock(key_size, query_size, value_size, num_hiddens,\n",
    "                             norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                             num_heads, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放，\n",
    "        # 然后再与位置编码相加。\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "\n",
    "        # X 是经过 Encoder 汇聚之后的结果\n",
    "        # X 包含了所有的输入信息，但又不是全都包含在了一个像是 RNN 的 state 矩阵中\n",
    "        # 把 X 丢给 Encoder 的第二个注意力层后，由这个注意力层来选择使用哪些信息\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中第i个块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i  # 存一下自己是第几个块\n",
    "        self.attention1 = d2l.MultiHeadAttention(\n",
    "            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)  # norm 应该等于 num_hiddens 吧\n",
    "        self.attention2 = d2l.MultiHeadAttention(\n",
    "            key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,  # ffn_num_input 应该等于 num_hiddens 把\n",
    "                                   num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)  # norm_shape 应该等于 num_hiddens 吧\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X (batch_size, num_steps, embedding_size=num_hiddens)\n",
    "        # decoder_block 拿到的输入 X 是原始样本经过 embedding 后的产物\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # 训练阶段，输出序列的所有词元都在同一时间处理，\n",
    "        # 因此state[2][self.i]初始化为None。\n",
    "        # 预测阶段，输出序列是通过词元一个接着一个解码的，\n",
    "        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示\n",
    "\n",
    "        # 以下这段注释可能有点毛病，注意分别是第i号个 Decoder-Block 还是第i个 Decoder-Block\n",
    "\n",
    "        # state[2] 中的 [self.i] 并不是代表第 i 个时刻的东西\n",
    "        # 而是 第i号个 Decoder 块的输入数据\n",
    "        # 在第 0 号时间步的数据进行前向运算的时候 state[2][...] 所有位置都是 None\n",
    "        # 因此在计算第 0 号时间步前向运算时，是第一次在 state[2][...] 中填充内容(对于每个样本序列而言，仅填充了一行)\n",
    "        # 接着计算后边的时间步的数值进行前向运算的时候 只需要把当前层的输入往下进行拼接即可\n",
    "\n",
    "        # state[2] 对于训练的过程没有什么帮助\n",
    "        # 在预测的时候才有帮助，因为预测的时候，每个时间步产生的每一个 Decoder-Block 的输出是一个时间步一个时间步产生的\n",
    "        # 而每次让 q 和所有的 k 做运算的时候，是需要把之前上一层的 Decoder-Block 产生的内容作为下一层 Decoder-Block 中\n",
    "        # 的 Attention 机制的 key-values pairs 的输入的，所以需要把之前时间步产生的 key-value 对给保存下来\n",
    "        \n",
    "        # 这同时也解释了，为什么在预测的时候不需要 valid_lens, 因为在 state[2] 中保存的 key_value 也就只有那么点\n",
    "        # 也就只有目前能够看到的序列产生的输出作为 key_value pairs 而后边的 key_value pairs 根本没存进去呢\n",
    "        \n",
    "        # 另外也能够解释为什么在训练的时候需要使用 valid_lens，因为在训练的时候一次性的，所有时间步的值都可以并行的作为输入\n",
    "        # 而 Self-Attention 计算第 i 个 q 的输出并不需要非得把前边的那些 q 都计算完了才能计算第 i 个 q\n",
    "        # 仅需要有所有的 key_values pairs 就可以计算任意一个 q 产生的输出了，这个不像 RNN 只能串行计算，而是可以并行计算\n",
    "        # 因此，一次性就输入进模型所有的 key_values pairs 了，这时候计算第 i 个 q 的时候与每一个 k 都进行了 dot product\n",
    "        # 但是，按照预测的逻辑来看，计算第 i 个 q 的 dot product 的时候，i 到 num_steps 的这些 q 产生的输出还不存在呢\n",
    "        # 但是在训练的时候这些 q 却可以被看到，因此就需要 valid_lens 来指明当前的 q 只能看到前多少个 key_values pairs\n",
    "\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if self.training:  # 应该是 net.train() 起的作用；把 self.training 设置为 True\n",
    "            batch_size, num_steps, _ = X.shape  # _ 是 embedding_size\n",
    "            # dec_valid_lens的开头:(batch_size,num_steps),\n",
    "            # 其中每一行是[1,2,...,num_steps]\n",
    "            # dec_valid_lens 是一个矩阵\n",
    "            # 做注意力机制的时候，每一个序列中的第 i 个 q 只能和这个序列的前 i 个 k 进行得分计算\n",
    "            # 因此就是 [1,2,...,num_steps]\n",
    "            dec_valid_lens = torch.arange(\n",
    "                1, num_steps + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "\n",
    "        # 自注意力\n",
    "        # 做注意力机制的时候，每一个序列中的第 i 个 q 只能和这个序列的前 i 个 k 进行得分计算\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # 编码器－解码器注意力。\n",
    "        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)\n",
    "decoder_blk.eval()\n",
    "X = torch.ones((2, 100, 24))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i),\n",
    "                DecoderBlock(key_size, query_size, value_size, num_hiddens,\n",
    "                             norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                             num_heads, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):  # i 代表的是第 i 号块，而 state[2][i] 确实是第 i 个块的输入，且是第 i-1 号块的输出\n",
    "            X, state = blk(X, state)\n",
    "            # 解码器自注意力权重\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # “编码器－解码器”自注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def read_data_nmt():\n",
    "    \"\"\"Load the English-French dataset.\n",
    "\n",
    "    Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "    data_dir = '/Users/blueberry/git-Library/Dive-into-Deep-Learning/10-Attention-Mechanisms-and-Transformers/data/'\n",
    "    with open(os.path.join(data_dir, 'seq.data'), 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\n",
    "\n",
    "    Defined in :numref:`subsec_mt_data_loading`\"\"\"\n",
    "    text = d2l.preprocess_nmt(read_data_nmt())\n",
    "    source, target = d2l.tokenize_nmt(text, num_examples)\n",
    "    src_vocab = d2l.Vocab(source, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = d2l.build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = d2l.build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [7, 5, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([2, 4]), tensor([[5, 0, 0, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([5, 2])]\n",
      "<unk>\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "data_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=20, num_examples=20)\n",
    "for batch in data_iter:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "print(src_vocab.idx_to_token[0])\n",
    "print(src_vocab['<pad>'])\n",
    "print(src_vocab['<bos>'])\n",
    "print(src_vocab['<eos>'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]  # 对最后一个维度且长度为 32 的向量做 LayerNorm 操作\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps, num_examples=600)\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    len(src_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'认识'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.idx_to_token[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器的输入是一个二维的样本矩阵 X 每一行都是一个句子\n",
    "# 解码器的输入 dec_input 也是一个样本矩阵，每一行都是一个句子并且有开始标志 <bos> 是在训练的时候添加上的\n",
    "d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])  and  torch.Size([64, 10])\n",
      "tensor([[ 5,  0,  4,  3,  1,  1,  1,  1,  1,  1],\n",
      "        [67,  7, 20, 92,  3,  1,  1,  1,  1,  1],\n",
      "        [ 0,  3,  1,  1,  1,  1,  1,  1,  1,  1]])\n",
      "tensor([[301,   0,   3,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [  9,   3,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "        [287,   0,   7,  43,   3,   1,   1,   1,   1,   1]])\n",
      "<pad>\n",
      "<pad>\n",
      "tensor([4, 5, 2])\n",
      "tensor([3, 2, 5])\n",
      "1\n",
      "1\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "for X, X_valid_lens, Y, Y_valid_lens in train_iter:\n",
    "    print(X.shape, ' and ', Y.shape)\n",
    "    print(X[:3, :])\n",
    "    print(Y[:3, :])\n",
    "    print(src_vocab.idx_to_token[1])\n",
    "    print(tgt_vocab.idx_to_token[1])\n",
    "    print(X_valid_lens[:3])\n",
    "    print(Y_valid_lens[:3])\n",
    "    break\n",
    "\n",
    "print(src_vocab['<pad>'])\n",
    "print(tgt_vocab['<pad>'])\n",
    "print(src_vocab.idx_to_token[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算 BLEU\n",
    "\n",
    "model.eval()\n",
    "\n",
    "作用：\n",
    "主要是针对model 在训练时和评价时不同的 Batch Normalization 和 Dropout 方法模式。\n",
    "Batch Normalization\n",
    "其作用对网络中间的每层进行归一化处理，并且使用变换重构（Batch Normalization Transform）保证每层提取的特征分布不会被破坏。训练时是针对每个mini-batch的，但是测试是针对单张图片的，即不存在batch的概念。由于网络训练完成后参数是固定的，每个batch的均值和方差是不变的，因此直接结算所有batch的均值和方差。所有Batch Normalization的训练和测试时的操作不同。\n",
    "Dropout\n",
    "其作用克服Overfitting，在每个训练批次中，通过忽略一半的特征检测器，可以明显的减少过拟合现象。\n",
    "\n",
    "model.eval()\n",
    "不启用 BatchNormalization 和 Dropout，保证BN和dropout不发生变化，pytorch框架会自动把BN和Dropout固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层影响结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呵呵 => 还 做饭 嘛 敷衍 嘛 嘛 敷衍 嘛 啦 不要\n",
      "有 什么 好 听的歌 => 还 做饭 嘛 敷衍 嘛 敷衍 嘛 啦 不要 考\n",
      "我 不 喜欢 你 了 => 还 做饭 只是 → 认识 哟 哟 东西 应该 打\n"
     ]
    }
   ],
   "source": [
    "engs = ['呵呵', \"有 什么 好 听的歌\", '我 不 喜欢 你 了']\n",
    "fras = ['是 王若 猫 的', '今夜 的 你 应该 明 了', '你 敢 我 就 敢 小样 敢 跟 我 叫板']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = d2l.predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}')\n",
    "    # print(f'{eng} => {translation}, ',\n",
    "    #       f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_attention_weights = torch.cat(net.encoder.attention_weights, 0).reshape((num_layers, num_heads,\n",
    "#     -1, num_steps))\n",
    "# enc_attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l.show_heatmaps(\n",
    "#     enc_attention_weights.cpu(), xlabel='Key positions',\n",
    "#     ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],\n",
    "#     figsize=(7, 3.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 Layer-Norm 和 Batch-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1, 2],\n",
    "    [2, 3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 传入一个整数 2 \n",
    "# 则参数被看作是一个只有一个元素 2 的 list\n",
    "# 代表对最后一个维度上长度为 2 的每个向量分别做归一化\n",
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000,  1.0000],\n",
       "         [-1.0000,  1.0000]], grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-1.0000, -1.0000],\n",
       "         [ 1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer-Norm 是把矩阵中的每一行归一化（每行是一个样本中一个数值的向量表示）\n",
    "# Batch-Norm 是把矩阵中的每一列归一化（每列是一个样本中多个数值的相同特征）\n",
    "# 因此 波波老师 的视频中讲的 均值方差归一化 属于是 Batch-Norm\n",
    "ln(X), bn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var = torch.mean(X[0]), torch.var(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.), tensor(1.))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-mean) / var, (2-mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var = torch.mean(X[1]), torch.var(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.), tensor(1.))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2-mean) / var, (3-mean) / var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 4],\n",
       "        [2, 3, 5]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [1, 2, 4],\n",
    "    [2, 3, 5]\n",
    "])\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4153,  0.5276, -1.9784,  0.9350, -0.7800, -0.1674, -0.7042],\n",
       "          [-1.7300,  0.3009, -2.6297, -0.5739,  0.5395,  0.7383,  1.5381],\n",
       "          [-0.6869,  1.2282, -0.6367,  1.7000, -1.3793,  0.9523,  1.5388]],\n",
       " \n",
       "         [[-1.7300,  0.3009, -2.6297, -0.5739,  0.5395,  0.7383,  1.5381],\n",
       "          [-1.1815,  0.7597, -1.4607,  0.3852, -0.6179, -0.3540,  0.1089],\n",
       "          [ 0.3327,  0.6494,  0.4247, -1.3969,  0.2311, -0.0742,  1.2649]]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([2, 3, 7]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "num_hiddens = 7\n",
    "\n",
    "\"\"\"\n",
    "embedding : (不确定内部是不是这样实现的)\n",
    "先对输入的序列中的每一个值做 one-hot\n",
    "在把每一个 one-hot 转化成一个连续类型的向量（做了一个全连接层）\n",
    "\n",
    "让 embedding 层学习到词和词之间的关系\n",
    "经过 embedding 层输出的是一个实数向量\n",
    "这样的一个向量表示的肯定不只是曾经 one-hot 表示的单个词的信息了\n",
    "从而就学习到了词和词之间的关系（联系）信息\n",
    "而且 embedding 的输出也不必和 词元个数(vocab_size) 相同，可以比 vocab_size 大，\n",
    "因为它承载了更多的内容\n",
    "\"\"\"\n",
    "\n",
    "# 通过 Embedding 的方式把句子中的每一个词元向量化表示\n",
    "embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "embedding(inputs), embedding(inputs).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 2,  2,  3,  4],\n",
       "         [ 2,  6,  7,  8]],\n",
       "\n",
       "        [[ 1,  2,  3,  4],\n",
       "         [ 1,  2,  3,  4],\n",
       "         [22,  2,  3,  4],\n",
       "         [22,  2,  3,  4]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = torch.tensor([\n",
    "[\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "],\n",
    "[\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2, 3, 4]\n",
    "]\n",
    "])\n",
    "\n",
    "X2 = torch.tensor([\n",
    "[\n",
    "    [2, 2, 3, 4],\n",
    "    [2, 6, 7, 8]\n",
    "],\n",
    "[\n",
    "    [22, 2, 3, 4],\n",
    "    [22, 2, 3, 4]\n",
    "]\n",
    "])\n",
    "\n",
    "# 在第 1 号轴的方向上进行拼接，横着拼\n",
    "# 对应模型中的 key_values 也是三维的\n",
    "# (batch_size, num_key_values_pairs, num_hiddens)\n",
    "# 每次都扩展 key_values，把之前输入的内容 concat 到 axis=1 上去\n",
    "res = torch.cat((X1, X2), axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 4]), torch.Size([2, 2, 4]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape, X2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 torch.arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2],\n",
       "        [1, 2],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成 1 到 3 之间的整数作为一个向量\n",
    "# 把这个向量在 0 号维度复制 4 次 得到一个矩阵\n",
    "# 把这个矩阵在 1 号维度复制 1 次 拼接起来\n",
    "dec_valid_lens = torch.arange(1, 3).repeat(4, 1)\n",
    "dec_valid_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86bb26bcf3701c754d5fd40db657c5b7e15a074c1099b32f8fc32052c18ad091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
