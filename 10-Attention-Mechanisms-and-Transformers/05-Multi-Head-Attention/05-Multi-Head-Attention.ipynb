{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensor.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12*2)\n",
    "X = X.reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]],\n",
       "\n",
       "        [[12, 16, 20],\n",
       "         [13, 17, 21],\n",
       "         [14, 18, 22],\n",
       "         [15, 19, 23]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在 X 的 1 和 2 的维度上进行转置\n",
    "# 写成 X.transpose(2, 1) 也是等价的\n",
    "# transpose 只是表明再哪两个维度所组成的矩阵上转置\n",
    "# 并不能表明转之后的顺序\n",
    "# 转置只能对矩阵进行\n",
    "X.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\n",
    "\n",
    "    Defined in :numref:`sec_attention-scoring-functions`\"\"\"\n",
    "    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n",
    "    # X (batch_size, num_steps, vector_size)\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            # 给定的 valid_lens 是一个矩阵\n",
    "            # 那么 对于第 j 个序列 且 第 i 个 q，只要前 valid_lens[j][i] 个得分\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        # value=-1e6 是一个绝对值特别大的负数，即 -100 万\n",
    "        # 经过一层这样的 mask 之后再计算 softmax 就可以让对应位置的概率得 0 了\n",
    "        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                              value=-1e6)\n",
    "\n",
    "        # 在 sequence_mask 之前是先把三维的数据展平成了一个二维的数据\n",
    "        # 让 三维 数据中后两个维度组成的矩阵按照第 1 个维度的顺序依次向下堆叠拼接\n",
    "        # 然后再使用 sequence_mask 对它进行 mask 操作\n",
    "        # 最后 对 mask 之后的数据进行一个 softmax 运算\n",
    "        # dim = -1 就是在最后一个维度所组成的向量上进行 softmax 运算\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\n",
    "\n",
    "    Defined in :numref:`subsec_additive-attention`\"\"\"\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n",
    "    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n",
    "    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n",
    "    # dimension)\n",
    "    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Set `transpose_b=True` to swap the last two dimensions of `keys`\n",
    "        # 转之后就可以让 queries 中的每一个 batch 且 batch 中每一个长度为 d 的行向量\n",
    "        # 与右侧的 每一个 batch 中的每一个长度为 d 的列向量进行运算\n",
    "        # 矩阵和矩阵的运算，然后堆叠在一起形成一个三位的输出结果\n",
    "        # tip: 每个矩阵是一个样本 batch_size 个矩阵就代表有 batch_size 个样本\n",
    "        # 而序列类型处理中关注的是当前序列的前后关系，并不关注当前序列中某个元素和其他序列中的关系\n",
    "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
    "        print('scores.shape is ', scores.shape)\n",
    "        print(scores)\n",
    "        # scores 的每一行是一个 q 向量和所有的 k 向量做完点积后的得分\n",
    "        # 所以 valid_lens 要指出每一行上的得分要前几个\n",
    "        # 如果 valid_lens 是一个矩阵并且每一行都是 vec = [1, 2, 3 ... num_steps]\n",
    "        # 这就代表对于第 i 个 q，它只要前 vec[i] 个得分做 softmax\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\n",
    "        # values 中某个样本矩阵中 每一行是一个 value 的向量表示\n",
    "        # 因此矩阵运算左侧的 attention_weight 矩阵每行都是对各个 value 向量的归一化后的分数\n",
    "        # attention_weight 的每行是一个概率分布，意义对应的乘在右侧的每个行向量上\n",
    "        # 最终加和在一起称为一个输出向量\n",
    "        print('attention_weights.shape is ', self.attention_weights.shape)\n",
    "        print(self.attention_weights)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，\n",
    "    # num_hiddens/num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    # num_hiddens/num_heads = 每个 head 的输入长度\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,\n",
    "    # num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "\n",
    "#@save\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "\n",
    "        # 按道理来说，每个 head 内部都应该有一个 W_q, W_k, W_v\n",
    "        # 这里为了数据处理方便，把每个 head 内部 科学系的矩阵合并在了一起\n",
    "        # 从而有了下方的这三个矩阵\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "\n",
    "        # W_o 用来规范化输出长度\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # queries，keys，values的形状:\n",
    "        # (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        # valid_lens　的形状:\n",
    "        # (batch_size，)或(batch_size，查询的个数)\n",
    "        # 经过变换后，输出的queries，keys，values　的形状:\n",
    "        # (batch_size*num_heads，查询或者“键－值”对的个数，\n",
    "        # num_hiddens/num_heads)\n",
    "        # 反正做 Self-Attention 的 q 和 k 配对的时候是对每个样本序列进行计算\n",
    "        # 并且在使用 DotProduct 并不会学习任何参数\n",
    "        # 因此在多个 head 中计算 q 和 k 使用的计算逻辑是一样的，但是输入的样本是不一样的\n",
    "        # 不妨把多个 head 的计算统计使用同一个计算模块来代替\n",
    "        # 这样一来就好像是有 num_heads * batch_size 这么多个样本一样 反正样本和样本之间也没有任何交集和干扰\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            # 因为每个 head 都需要一个 valid_lens\n",
    "            # 因此就复制 num_heads 次 valid_lens\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # output的形状:(batch_size*num_heads，查询的个数，\n",
    "        # num_hiddens/num_heads) num_hiddens/num_heads = 计算得分时 qkv 的向量长度\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "        # output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        # 这里的 output_concat 最后的一个维度是 num_hiddens\n",
    "        # 这就相当于把所有 head 的输出拼接在了一起，最后再丢给 W_o 做一个输出格式的变换\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores.shape is  torch.Size([10, 4, 6])\n",
      "tensor([[[ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754],\n",
      "         [ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754],\n",
      "         [ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754],\n",
      "         [ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754]],\n",
      "\n",
      "        [[-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142],\n",
      "         [-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142],\n",
      "         [-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142],\n",
      "         [-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142]],\n",
      "\n",
      "        [[ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615],\n",
      "         [ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615],\n",
      "         [ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615],\n",
      "         [ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615]],\n",
      "\n",
      "        [[ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407],\n",
      "         [ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407],\n",
      "         [ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407],\n",
      "         [ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407]],\n",
      "\n",
      "        [[-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869],\n",
      "         [-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869],\n",
      "         [-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869],\n",
      "         [-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869]],\n",
      "\n",
      "        [[ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754],\n",
      "         [ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754],\n",
      "         [ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754],\n",
      "         [ 0.2754,  0.2754,  0.2754,  0.2754,  0.2754,  0.2754]],\n",
      "\n",
      "        [[-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142],\n",
      "         [-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142],\n",
      "         [-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142],\n",
      "         [-0.7142, -0.7142, -0.7142, -0.7142, -0.7142, -0.7142]],\n",
      "\n",
      "        [[ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615],\n",
      "         [ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615],\n",
      "         [ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615],\n",
      "         [ 0.0615,  0.0615,  0.0615,  0.0615,  0.0615,  0.0615]],\n",
      "\n",
      "        [[ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407],\n",
      "         [ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407],\n",
      "         [ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407],\n",
      "         [ 0.1407,  0.1407,  0.1407,  0.1407,  0.1407,  0.1407]],\n",
      "\n",
      "        [[-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869],\n",
      "         [-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869],\n",
      "         [-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869],\n",
      "         [-0.0869, -0.0869, -0.0869, -0.0869, -0.0869, -0.0869]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "attention_weights.shape is  torch.Size([10, 4, 6])\n",
      "tensor([[[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "\n",
    "\n",
    "num_kvpairs, valid_lens =  6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2l.sequence_mask(X, valid_lens)[:, :, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 2, 2, 2])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 维度不变，把每个数值复制 n 次\n",
    "n = 3\n",
    "torch.repeat_interleave(valid_lens, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(1, 6).repeat(2, 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.repeat(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(X, 2, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax\n",
    "\n",
    "通过下方的一系列例子，可以表明 softmax 默认在最后一个维度上进行归一化，其中 dim 参数指明在哪个维度进行归一化，并且归一化后，值为 0 的情况并不会被归一化为 0，softmax 只是让小的更小，大的更大，这里的更小和更大只是让两者在差异上显得更大而已，但是为 0 的值归一化后却不是 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(2*3*4, dtype=torch.float32).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0321, 0.0871, 0.2369, 0.6439],\n",
       "         [0.0321, 0.0871, 0.2369, 0.6439],\n",
       "         [0.0321, 0.0871, 0.2369, 0.6439]],\n",
       "\n",
       "        [[0.0321, 0.0871, 0.2369, 0.6439],\n",
       "         [0.0321, 0.0871, 0.2369, 0.6439],\n",
       "         [0.0321, 0.0871, 0.2369, 0.6439]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(X, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0634, 0.4683, 0.4683])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([0, 2, 2], dtype=torch.float32), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([ 0.,  1.,  2.,  3.], dtype=torch.float32), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321, 0.0871, 0.2369, 0.6439])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([ 4.,  5.,  6.,  7.], dtype=torch.float32), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.1442e-06, 6.1442e-06, 6.1442e-06, 6.1442e-06],\n",
       "         [6.1442e-06, 6.1442e-06, 6.1442e-06, 6.1442e-06],\n",
       "         [6.1442e-06, 6.1442e-06, 6.1442e-06, 6.1442e-06]],\n",
       "\n",
       "        [[9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01],\n",
       "         [9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01],\n",
       "         [9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9999e-01]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(X, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.1442e-06, 9.9999e-01])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([0, 12], dtype=torch.float32), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.079"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.5079e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 理解 sequence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0, 0],\n",
       "        [1, 2, 3, 0],\n",
       "        [1, 2, 0, 0],\n",
       "        [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2, 3, 4],\n",
    "    [1, 2, 3, 4]\n",
    "])\n",
    "\n",
    "valid = torch.tensor([2, 3, 2, 1])\n",
    "\n",
    "d2l.sequence_mask(X, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86bb26bcf3701c754d5fd40db657c5b7e15a074c1099b32f8fc32052c18ad091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
